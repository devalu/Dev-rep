The following factors led to the selection of linear regression:

Simplicity: The model provided by linear regression is easy to understand and offers insights into the relationship between features and the target variable.

Assumption of Linearity: When the assumption of a linear relationship between the features and the target variable is met, Linear Regression is appropriate.

Implementation Ease: It is suited for contexts with limited resources and massive datasets because it is computationally efficient and simple to implement.

Baseline Model: When evaluating the effectiveness of increasingly sophisticated algorithms, Linear Regression is used as the baseline model.

Interpretability: In linear regression, coefficients shed light on how significant a characteristic is in forecasting the dependent variable.

Overall, the selection of linear regression was based on its interpretability, simplicity, and applicability for linear correlations between the target and features. 

The following factors led to the selection of Random Forest:

Accuracy: Random Forest is a robust and highly accurate algorithm that works well for classification applications.

Ensemble learning aims to increase generalization performance by reducing overfitting by combining numerous decision trees.

Simple to Use: Random Forest works well with default settings and requires little hyperparameter tweaking.

Importance of Feature: It offers information on the most illuminating features for forecasting.

Interpretability: Random Forest provides some interpretability through feature importances even though it is an ensemble model.

In general, Random Forest was selected due to its excellent performance, user-friendliness, and capacity to manage intricate datasets.



